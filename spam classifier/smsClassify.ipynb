{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interview Practice\n",
    "# SMS Spam Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13406317300789664"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#1. Analyze data\n",
    "# load csv into numpy\n",
    "\n",
    "csv = pd.read_csv(\"spam.csv\",encoding='latin-1')\n",
    "label = csv.iloc[:,0]\n",
    "sms = csv.iloc[:,1]\n",
    "csv.head()\n",
    "\n",
    "# first look at the proportion of spam and ham\n",
    "spam=0\n",
    "ham=0\n",
    "\n",
    "for row in label:\n",
    "    if row=='ham':\n",
    "        ham+=1\n",
    "    else:\n",
    "        spam+=1\n",
    "\n",
    "spam_percent= spam/(spam+ham)\n",
    "ham_percent = ham/(spam+ham)\n",
    "\n",
    "\n",
    "#spam (0.13) is far less than ham (0.86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "#2. Feature extraction/ preprocess \n",
    "# note: I realize there's some miniscule amount of text in coulmn 3,4,5, ignored here for simplicity\n",
    "\n",
    "\n",
    "# tokenization, convert to lowercase, remove characters: .,*\n",
    "def tokenization(spam):\n",
    "    tokens={}\n",
    "    spam= spam.strip()\n",
    "    for token in re.split(r'[^A-Za-z0-9]+', spam):\n",
    "        token=token.lower()\n",
    "        if token not in tokens:\n",
    "            tokens[token]= 1\n",
    "        else:\n",
    "            tokens[token]+=1\n",
    "    return tokens\n",
    "\n",
    "word_matrix=[]\n",
    "for i in sms:\n",
    "    word_matrix.append(tokenization(i))\n",
    "\n",
    "# convert to sparse matrix\n",
    "v = DictVectorizer(sparse=False)\n",
    "X = v.fit_transform(word_matrix)\n",
    "    \n",
    "#split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Model selection\n",
    "# Naive bayes is commonly used for text classification\n",
    "# here just comparing 2 methods: multinomial and bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4 Model training\n",
    "# not sure about this, can use Sklearn or need to build the wheel myself?\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "\n",
    "Mnb = MultinomialNB(alpha=1)\n",
    "Mnb.fit(X, y)\n",
    "\n",
    "Bnb = BernoulliNB()\n",
    "Bnb.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomial accuracy:  0.9928251121076234\n",
      "bernoulli accuracy:  0.9883408071748879\n"
     ]
    }
   ],
   "source": [
    "#5 Model Evaluation\n",
    "\n",
    "multinomial_accuracy= Mnb.score(X_test, y_test)\n",
    "bernoulli_accuracy= Bnb.score(X_test, y_test)\n",
    "\n",
    "print('multinomial accuracy: ', multinomial_accuracy)\n",
    "print('bernoulli accuracy: ', bernoulli_accuracy)\n",
    "\n",
    "#since multinomial gives higher accuracy rate, we go for multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomial prediction for ham:  ['ham']\n",
      "multinomial prediction for spam:  ['spam']\n",
      "bernoulli prediction for ham:  ['ham']\n",
      "bernoulli prediction for spam:  ['ham']\n"
     ]
    }
   ],
   "source": [
    "#6 Prediction\n",
    "\n",
    "ham_feed= \"have a party tonight to celebrate winning the lottery, don't be late! Max\"\n",
    "spam_feed= \"congratulation! you won the US Mega Pot, claim it before august 21 by replying to 042346742\"\n",
    "\n",
    "\n",
    "predict_ham=v.transform([tokenization(ham_feed)])  \n",
    "predict_spam= v.transform([tokenization(spam_feed)])  \n",
    "\n",
    "\n",
    "print('multinomial prediction for ham: ', Mnb.predict(predict_ham))\n",
    "print('multinomial prediction for spam: ',Mnb.predict(predict_spam))\n",
    "\n",
    "print('bernoulli prediction for ham: ',Bnb.predict(predict_ham))\n",
    "print('bernoulli prediction for spam: ',Bnb.predict(predict_spam))\n",
    "\n",
    "# multinomial predicts both case correct, bernoulli only 1, multinomial more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 not sure about this, do they expect parameter tuning to build some graph? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
